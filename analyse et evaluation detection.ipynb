{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphiques et rapports générés avec succès.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from fpdf import FPDF\n",
    "from docx import Document\n",
    "import os\n",
    "\n",
    "# Chemins de sortie\n",
    "base_path = \"C:\\\\Users\\\\sbond\\\\Desktop\\\\SPAM Sms detection\"\n",
    "results_path = os.path.join(base_path, \"resultat et evaluation detection\")\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "# Charger les données\n",
    "data_path = \"C:\\\\Users\\\\sbond\\\\Desktop\\\\SPAM Sms detection\\\\SMSSpamCollection.csv\"\n",
    "data = pd.read_csv(data_path, sep='\\t', header=None, names=['label', 'message'])\n",
    "\n",
    "# Nettoyage des données\n",
    "def clean_message(message):\n",
    "    stopwords = set([\n",
    "        \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\n",
    "        \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\",\n",
    "        \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n",
    "        \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\",\n",
    "        \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\",\n",
    "        \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\",\n",
    "        \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\",\n",
    "        \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\",\n",
    "        \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\",\n",
    "        \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\",\n",
    "        \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\",\n",
    "        \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"\n",
    "    ])\n",
    "    message = ''.join([char for char in message.lower() if char.isalnum() or char.isspace()])\n",
    "    return ' '.join([word for word in message.split() if word not in stopwords])\n",
    "\n",
    "data['cleaned_message'] = data['message'].apply(clean_message)\n",
    "\n",
    "# Vectorisation et division des données\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(data['cleaned_message'])\n",
    "y = (data['label'] == 'spam').astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Entraînement et évaluation\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "classification_rep = classification_report(y_test, y_pred, target_names=[\"ham\", \"spam\"], output_dict=True)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Sauvegarde des graphiques\n",
    "def save_graphs():\n",
    "    # Distribution des classes\n",
    "    class_distribution = data['label'].value_counts()\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    bars = ax.bar(class_distribution.index, class_distribution.values, color=['blue', 'orange'])\n",
    "    plt.title('Distribution des classes (HAM vs SPAM)')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.ylabel('Nombre de messages')\n",
    "    plt.xlabel('Label')\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2.0, height, f'{int(height)}', ha='center', va='bottom')\n",
    "    plt.savefig(os.path.join(results_path, \"distribution_classes.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Matrice de confusion\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    cax = ax.matshow(conf_matrix, cmap=\"Blues\")\n",
    "    plt.colorbar(cax)\n",
    "    plt.title('Matrice de confusion')\n",
    "    plt.xlabel('Prédictions')\n",
    "    plt.ylabel('Classe réelle')\n",
    "    plt.xticks([0, 1], ['HAM', 'SPAM'])\n",
    "    plt.yticks([0, 1], ['HAM', 'SPAM'])\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        for j in range(conf_matrix.shape[1]):\n",
    "            ax.text(j, i, str(conf_matrix[i, j]), ha='center', va='center', color='red')\n",
    "    plt.savefig(os.path.join(results_path, \"matrice_confusion.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Longueur moyenne des messages\n",
    "    data['message_length'] = data['message'].apply(len)\n",
    "    avg_length = data.groupby('label')['message_length'].mean()\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    bars = ax.bar(avg_length.index, avg_length.values, color=['blue', 'orange'])\n",
    "    plt.title('Longueur moyenne des messages par classe')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.ylabel('Longueur moyenne')\n",
    "    plt.xlabel('Classe')\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2.0, height, f'{height:.2f}', ha='center', va='bottom')\n",
    "    plt.savefig(os.path.join(results_path, \"message_length.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Mots les plus fréquents dans les spams\n",
    "    spam_messages = data[data['label'] == 'spam']['cleaned_message']\n",
    "    vectorizer_count = CountVectorizer(max_features=10)\n",
    "    frequent_words = vectorizer_count.fit_transform(spam_messages)\n",
    "    word_counts = np.asarray(frequent_words.sum(axis=0)).flatten()\n",
    "    words = vectorizer_count.get_feature_names_out()\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    bars = ax.bar(words, word_counts, color='orange')\n",
    "    plt.title('Mots les plus fréquents dans les SPAM')\n",
    "    plt.ylabel('Fréquence')\n",
    "    plt.xlabel('Mots')\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2.0, height, f'{int(height)}', ha='center', va='bottom')\n",
    "    plt.savefig(os.path.join(results_path, \"frequent_words_spam.png\"))\n",
    "    plt.close()\n",
    "\n",
    "save_graphs()\n",
    "\n",
    "# Génération des rapports PDF et Word\n",
    "def save_reports():\n",
    "    # Rapport PDF\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    pdf.cell(200, 10, txt=\"Rapport de Détection de SPAM SMS\", ln=True, align=\"C\")\n",
    "    pdf.ln(10)\n",
    "\n",
    "    # Ajouter les graphiques\n",
    "    pdf.set_font(\"Arial\", size=10)\n",
    "    pdf.cell(0, 10, \"Graphiques :\", ln=True)\n",
    "    pdf.ln(5)\n",
    "    for graph in [\"distribution_classes.png\", \"matrice_confusion.png\", \"message_length.png\", \"frequent_words_spam.png\"]:\n",
    "        pdf.image(os.path.join(results_path, graph), x=10, w=180)\n",
    "        pdf.ln(5)\n",
    "\n",
    "    # Ajouter la matrice de confusion sous forme de tableau\n",
    "    pdf.cell(0, 10, \"Matrice de Confusion :\", ln=True)\n",
    "    pdf.cell(40, 10, \"Classe Réelle\", border=1, align='C')\n",
    "    pdf.cell(40, 10, \"Prédit HAM\", border=1, align='C')\n",
    "    pdf.cell(40, 10, \"Prédit SPAM\", border=1, align='C')\n",
    "    pdf.ln()\n",
    "    for i, row in enumerate(conf_matrix):\n",
    "        pdf.cell(40, 10, \"HAM\" if i == 0 else \"SPAM\", border=1, align='C')\n",
    "        pdf.cell(40, 10, str(row[0]), border=1, align='C')\n",
    "        pdf.cell(40, 10, str(row[1]), border=1, align='C')\n",
    "        pdf.ln()\n",
    "\n",
    "    pdf.ln(10)\n",
    "\n",
    "    # Ajouter les métriques de classification sous forme de tableau\n",
    "    pdf.cell(0, 10, \"Métriques de Classification :\", ln=True)\n",
    "    pdf.cell(50, 10, \"Classe\", border=1, align='C')\n",
    "    pdf.cell(50, 10, \"Précision\", border=1, align='C')\n",
    "    pdf.cell(50, 10, \"F1-Score\", border=1, align='C')\n",
    "    pdf.ln()\n",
    "    for label, metrics in classification_rep.items():\n",
    "        if isinstance(metrics, dict):\n",
    "            pdf.cell(50, 10, label, border=1, align='C')\n",
    "            pdf.cell(50, 10, f\"{metrics['precision']:.2f}\", border=1, align='C')\n",
    "            pdf.cell(50, 10, f\"{metrics['f1-score']:.2f}\", border=1, align='C')\n",
    "            pdf.ln()\n",
    "\n",
    "    pdf.output(os.path.join(results_path, \"rapport_detection_spam.pdf\"))\n",
    "\n",
    "    # Rapport Word\n",
    "    doc = Document()\n",
    "    doc.add_heading(\"Rapport de Détection de SPAM SMS\", level=1)\n",
    "\n",
    "    # Ajouter les graphiques\n",
    "    doc.add_heading(\"Graphiques :\", level=2)\n",
    "    for graph in [\"distribution_classes.png\", \"matrice_confusion.png\", \"message_length.png\", \"frequent_words_spam.png\"]:\n",
    "        doc.add_picture(os.path.join(results_path, graph), width=doc.sections[0].page_width * 0.8)\n",
    "\n",
    "    # Matrice de confusion\n",
    "    doc.add_heading(\"Matrice de Confusion\", level=2)\n",
    "    table = doc.add_table(rows=3, cols=3)\n",
    "    table.style = 'Table Grid'\n",
    "    table.cell(0, 0).text = \"Classe Réelle\"\n",
    "    table.cell(0, 1).text = \"Prédit HAM\"\n",
    "    table.cell(0, 2).text = \"Prédit SPAM\"\n",
    "    for i, row in enumerate(conf_matrix):\n",
    "        table.cell(i + 1, 0).text = \"HAM\" if i == 0 else \"SPAM\"\n",
    "        table.cell(i + 1, 1).text = str(row[0])\n",
    "        table.cell(i + 1, 2).text = str(row[1])\n",
    "\n",
    "    # Métriques de classification\n",
    "    doc.add_heading(\"Métriques de Classification\", level=2)\n",
    "    table = doc.add_table(rows=len(classification_rep) + 1, cols=3)\n",
    "    table.style = 'Table Grid'\n",
    "    table.cell(0, 0).text = \"Classe\"\n",
    "    table.cell(0, 1).text = \"Précision\"\n",
    "    table.cell(0, 2).text = \"F1-Score\"\n",
    "    for i, (label, metrics) in enumerate(classification_rep.items()):\n",
    "        if isinstance(metrics, dict):\n",
    "            table.cell(i + 1, 0).text = label\n",
    "            table.cell(i + 1, 1).text = f\"{metrics['precision']:.2f}\"\n",
    "            table.cell(i + 1, 2).text = f\"{metrics['f1-score']:.2f}\"\n",
    "\n",
    "    doc.save(os.path.join(results_path, \"rapport_detection_spam.docx\"))\n",
    "\n",
    "save_reports()\n",
    "\n",
    "print(\"Graphiques et rapports générés avec succès.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
